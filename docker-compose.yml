

services:
  ollama_server:
    build:
      context: ./docker/ollama
    container_name: ollama_server
    restart: unless-stopped # Restart the container if it crashes
    pull_policy: always # Para asegurar que se descarguen los modelos
    tty: true # Para poder usar el contenedor como un terminal
    ports:
      - "11434:11434"       # Puerto de la API REST
    volumes:
      - ollama_data:/root/.ollama  # Persistir modelos y configuraciones
    # Activa esta l√≠nea si tienes GPU Nvidia y NVIDIA Container Toolkit instalado
    runtime: nvidia
    deploy:
       resources:
         reservations:
           devices:
             - driver: nvidia
               count: 1
               capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all   # Si usas GPU; permite ver todas las GPUs
    #command: ["sh", "-c", "ollama"]

    

  fastapi_app:
    build:
      context: .
      dockerfile: docker/fastapi/Dockerfile
    container_name: fastapi_container
    ports:
      - "8000:8000"  # Host:Contenedor
    #command: uvicorn main:app --host 0.0.0.0 --port 8000
    env_file:
      - .env
    environment:
      - DEEPSEEK_URL=http://ollama_server:11434
    depends_on:
      - ollama_server

volumes:
  ollama_data:
