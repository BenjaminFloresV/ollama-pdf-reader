

services:
  ollama_server:
    build:
      context: ./docker/ollama
    container_name: ollama_server
    restart: unless-stopped # Restart the container if it crashes
    pull_policy: always # Para asegurar que se descarguen los modelos
    tty: true # Para poder usar el contenedor como un terminal
    ports:
      - "11434:11434"       # Puerto de la API REST
    volumes:
      - ollama_data:/root/.ollama  # Persistir modelos y configuraciones
    # Activa esta l√≠nea si tienes GPU Nvidia y NVIDIA Container Toolkit instalado
    runtime: nvidia
    deploy:
       resources:
         reservations:
           devices:
             - driver: nvidia
               count: 1
               capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all   # Si usas GPU; permite ver todas las GPUs
    #command: ["sh", "-c", "ollama"]

    

  fastapi_app:
    build:
      context: .
      dockerfile: docker/fastapi/Dockerfile
    container_name: fastapi_container
    ports:
      - "8000:8000"  # Host:Contenedor
    #command: uvicorn main:app --host 0.0.0.0 --port 8000
    env_file:
      - .env
    environment:
      - MONGO_URI=mongodb://${MONGO_USER}:${MONGO_PASSWORD}@mongo:27017/?authSource=admin
      - OLLAMA_URL=http://ollama_server:11434
    depends_on:
      - ollama_server
      #- mongo

  #mongo:
  #  image: mongo
  #  environment:
  #    - MONGO_INITDB_ROOT_USERNAME=${MONGO_USER}
  #    - MONGO_INITDB_ROOT_PASSWORD=${MONGO_PASSWORD}
  #  ports:
  #    - "27017:27017"
  #  volumes:
  #    - mongo_data:/data/db

volumes:
  ollama_data:
  mongo_data:
